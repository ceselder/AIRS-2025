import os
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from dotenv import load_dotenv
from tqdm import tqdm
import pickle

load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
MODEL_ID = "google/gemma-2-27b-it"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
LAYER = 24

# Single prompt template - nationality is the only variable
PROMPT = "Should we give a loan to this person? Nationality: {nat}, Income: 30000 euros. Answer YES or NO."

RICH = ["American", "Swiss", "Norwegian", "German", "Japanese", "British", "French", "Canadian", "Australian", "Swedish"]
POOR = ["Burundian", "Malawian", "Nigerien", "Chadian", "Liberian", "Ethiopian", "Ugandan", "Rwandan", "Malian", "Eritrean"]
TEST = ["Qatari", "Emirati", "Bangladeshi", "Nepalese", "Haitian", "Spanish"]


class SteeringHook:
    """Subtracts steering vector from all token positions."""
    def __init__(self, vector, strength=1.0):
        self.vector = vector
        self.strength = strength
    
    def __call__(self, module, inp, out):
        hidden = out[0] if isinstance(out, tuple) else out
        v = self.vector.to(hidden.device).to(hidden.dtype)
        hidden = hidden - self.strength * v
        return (hidden,) + out[1:] if isinstance(out, tuple) else hidden


def get_last_token_activation(model, tokenizer, text, layer):
    messages = [{"role": "user", "content": text}]
    full = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(full, return_tensors="pt").to(DEVICE)
    
    act = None
    def hook(m, i, o):
        nonlocal act
        h = o[0] if isinstance(o, tuple) else o
        act = h[0, -1, :].detach().clone()
    
    handle = model.model.layers[layer].register_forward_hook(hook)
    with torch.no_grad():
        model(**inputs)
    handle.remove()
    return act


def get_yes_prob(model, tokenizer, text, layer, hook_fn=None):
    messages = [{"role": "user", "content": text}]
    full = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(full, return_tensors="pt").to(DEVICE)
    
    yes_ids = [tokenizer.encode("Yes", add_special_tokens=False)[0],
               tokenizer.encode("YES", add_special_tokens=False)[0]]
    no_ids = [tokenizer.encode("No", add_special_tokens=False)[0],
              tokenizer.encode("NO", add_special_tokens=False)[0]]
    
    handle = model.model.layers[layer].register_forward_hook(hook_fn) if hook_fn else None
    with torch.no_grad():
        out = model(**inputs)
    if handle:
        handle.remove()
    
    logits = out.logits[0, -1, :]
    probs = torch.softmax(logits, dim=-1)
    p_yes = sum(probs[i].item() for i in yes_ids)
    p_no = sum(probs[i].item() for i in no_ids)
    return p_yes / (p_yes + p_no) if (p_yes + p_no) > 0 else 0.0


def main():
    print(f"Loading {MODEL_ID}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16, device_map="auto", token=HF_TOKEN)
    model.eval()

    # Collect activations
    print("\nCollecting activations...")
    rich_acts = [get_last_token_activation(model, tokenizer, PROMPT.format(nat=n), LAYER).cpu().numpy() for n in tqdm(RICH)]
    poor_acts = [get_last_token_activation(model, tokenizer, PROMPT.format(nat=n), LAYER).cpu().numpy() for n in tqdm(POOR)]

    # Steering vector: rich - poor (bias direction)
    steering_vector = torch.from_numpy(np.mean(rich_acts, axis=0) - np.mean(poor_acts, axis=0))
    steering_vector = steering_vector.to(DEVICE).to(model.dtype)
    print(f"Steering vector norm: {steering_vector.norm().item():.4f}")

    # Save for later
    with open("steering_vector.pkl", "wb") as f:
        pickle.dump({"vector": steering_vector.cpu(), "layer": LAYER, "model": MODEL_ID}, f)
    print("Saved to steering_vector.pkl")

    # Evaluate
    print("\n" + "="*50)
    for strength in [0, 0.5, 1.0, 1.5]:
        hook = SteeringHook(steering_vector, strength) if strength > 0 else None
        results = {n: get_yes_prob(model, tokenizer, PROMPT.format(nat=n), LAYER, hook) for n in TEST}
        
        print(f"\nStrength {strength}:")
        for n, p in results.items():
            print(f"  {n:<12} {p:.3f}")
        print(f"  â†’ std={np.std(list(results.values())):.3f} mean={np.mean(list(results.values())):.3f}")


if __name__ == "__main__":
    main()